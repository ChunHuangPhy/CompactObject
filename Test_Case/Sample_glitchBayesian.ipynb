{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "from scipy.constants import pi\n",
    "from scipy.integrate import ode\n",
    "import scipy.stats\n",
    "from scipy import optimize\n",
    "from itertools import repeat\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from scipy import stats\n",
    "from numba import njit, jit, float64\n",
    "import ultranest\n",
    "import random\n",
    "#Package we need:\n",
    "import InferenceWorkflow.BayesianSampler as sampler\n",
    "import InferenceWorkflow.Likelihood as likelihood\n",
    "\n",
    "import InferenceWorkflow.prior as prior\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "data = [(1.08, 112), (2.43, 150), (1.133, 116), (0.2, 25), (0.855, 53),\n",
    "(0.94, 70), (0.151, 8), (6.595, 250), (0.65, 30), (1.46, 132)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_x = np.array(data).T[0]\n",
    "data_y = np.array(data).T[1]\n",
    "uncertainty = np.array([2,5,5,3,3,10,2,20,5,5])\n",
    "\n",
    "def prior_transform(cube):\n",
    "    params = cube.copy()\n",
    "    params[0] = prior.flat_prior(0, 300,cube[0]) #Nq=9,12,15,18,21,24\n",
    "    params[1] = prior.flat_prior(0,3,cube[1]) #xxepsilon=5-170MeV  ;  \n",
    "    return params\n",
    "\n",
    "def likelihood_transform(theta):\n",
    "   \n",
    "    k, nu = theta # comment this line if you need two measuremnts.\n",
    "    y_th = k*data_x**nu\n",
    "\n",
    "    prob = np.sum(np.log(1/(uncertainty*(np.sqrt(2*np.pi))**2)*np.exp(-np.power(y_th-data_y, 2.)/(2*np.power(uncertainty,2.)))))\n",
    "\n",
    "    if prob == -np.inf:\n",
    "        prob = -1e101\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultranest\n",
    "import ultranest.stepsampler\n",
    "parameters = ['k','nu']\n",
    "step = 2 * len(parameters)\n",
    "live_points = 50000\n",
    "\n",
    "sampler = ultranest.ReactiveNestedSampler(parameters, likelihood_transform,prior_transform,log_dir='output')\n",
    "sampler.stepsampler = ultranest.stepsampler.SliceSampler(\n",
    "        nsteps=step,\n",
    "        generate_direction=ultranest.stepsampler.generate_mixture_random_direction,\n",
    "        # adaptive_nsteps=False,\n",
    "        # max_nsteps=400\n",
    "    )\n",
    "\n",
    "result = sampler.run(min_num_live_points=live_points,max_ncalls= max_calls)\n",
    "flat_samples = sampler.results['samples']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import corner\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create labels for the parameters\n",
    "\n",
    "\n",
    "# Generate corner plot\n",
    "figure = corner.corner(\n",
    "    np.array(flat_samples),\n",
    "    labels=parameters, \n",
    "    show_titles=True,\n",
    "    title_kwargs={\"fontsize\": 12},\n",
    "    quantiles=[0.16, 0.5, 0.84],  # Show the 16th, 50th, and 84th percentiles  # Enable plotting of individual data points\n",
    "    fill_contours=True, \n",
    "    plot_datapoints=False,\n",
    "    bins=20,  # Increase the number of bins\n",
    "    smooth=0.9,  # Enable KDE smoothing\n",
    "    alpha=0.8  # Set alpha for better visualization of dense regions\n",
    ")\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "from scipy.constants import pi\n",
    "from scipy.integrate import ode\n",
    "import scipy.stats\n",
    "from scipy import optimize\n",
    "from itertools import repeat\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from scipy import stats\n",
    "from numba import njit, jit, float64\n",
    "import ultranest\n",
    "import random\n",
    "#Package we need:\n",
    "import InferenceWorkflow.BayesianSampler as sampler\n",
    "import InferenceWorkflow.Likelihood as likelihood\n",
    "\n",
    "import InferenceWorkflow.prior as prior\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "data = [(1.08, 112), (2.43, 150), (1.133, 116), (0.2, 25), (0.855, 53),\n",
    "(0.94, 70), (0.151, 8), (6.595, 250), (0.65, 30), (1.46, 132),(15.3+0.9388+0.188955,434)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_x = np.array(data).T[0]\n",
    "data_y = np.array(data).T[1]\n",
    "uncertainty = np.array([2,5,5,3,3,10,2,20,5,5,8])\n",
    "\n",
    "def prior_transform(cube):\n",
    "    params = cube.copy()\n",
    "    params[0] = prior.flat_prior(0, 300,cube[0]) #Nq=9,12,15,18,21,24\n",
    "    params[1] = prior.flat_prior(0,3,cube[1]) #epsilon=5-170MeV  ;  \n",
    "    return params\n",
    "\n",
    "def likelihood_transform(theta):\n",
    "   \n",
    "    k, nu = theta # comment this line if you need two measuremnts.\n",
    "    y_th = k*data_x**nu\n",
    "\n",
    "    prob = np.sum(np.log(1/(uncertainty*(np.sqrt(2*np.pi))**2)*np.exp(-np.power(y_th-data_y, 2.)/(2*np.power(uncertainty,2.)))))\n",
    "\n",
    "    if prob == -np.inf:\n",
    "        prob = -1e101\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultranest\n",
    "import ultranest.stepsampler\n",
    "parameters = ['k','nu']\n",
    "step = 2 * len(parameters)\n",
    "live_points = 50000\n",
    "\n",
    "max_calls = 60000000\n",
    "sampler = ultranest.ReactiveNestedSampler(parameters, likelihood_transform,prior_transform,log_dir='output')\n",
    "sampler.stepsampler = ultranest.stepsampler.SliceSampler(\n",
    "        nsteps=step,\n",
    "        generate_direction=ultranest.stepsampler.generate_mixture_random_direction,\n",
    "        # adaptive_nsteps=False,\n",
    "        # max_nsteps=400\n",
    "    )\n",
    "\n",
    "result = sampler.run(min_num_live_points=live_points,max_ncalls= max_calls)\n",
    "flat_samples = sampler.results['samples']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "from scipy.constants import pi\n",
    "from scipy.integrate import ode\n",
    "import scipy.stats\n",
    "from scipy import optimize\n",
    "from itertools import repeat\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from scipy import stats\n",
    "from numba import njit, jit, float64\n",
    "import ultranest\n",
    "import random\n",
    "#Package we need:\n",
    "import InferenceWorkflow.BayesianSampler as sampler\n",
    "import InferenceWorkflow.Likelihood as likelihood\n",
    "\n",
    "import InferenceWorkflow.prior as prior\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "data = [(1.08, 112), (2.43, 150), (1.133, 116), (0.2, 25), (0.855, 53),\n",
    "(0.94, 70), (0.151, 8), (6.595, 250), (0.65, 30), (1.46, 132)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_x = np.array(data).T[0]\n",
    "data_y = np.array(data).T[1]\n",
    "uncertainty = np.array([2,5,5,3,3,10,2,20,5,5])\n",
    "\n",
    "\n",
    "\n",
    "def prior_transform(cube):\n",
    "    params = cube.copy()\n",
    "    \n",
    "    params[0] = prior.flat_prior(0,0.01,cube[0]) #epsilon=5-170MeV  ;  \n",
    "    params[1] = prior.flat_prior(0,0.01,cube[1]) #epsilon=5-170MeV  ;\n",
    "    params[2] = prior.flat_prior(0,3,cube[2]) #epsilon=5-170MeV  ;\n",
    "    params[3] = prior.flat_prior(-0.2,0,cube[3]) #epsilon=5-170MeV  ;\n",
    "\n",
    "    return params\n",
    "def f_approx(x, gamma_1, gamma_2, alpha, xi):\n",
    "    # Terms in the Taylor series expansion\n",
    "    term1 = alpha**(-1) * x**gamma_1\n",
    "    term2 = -alpha**(-2) * xi * x**(gamma_1 + gamma_2)\n",
    "    term3 = alpha**(-3) * xi**2 * x**(gamma_1 + 2*gamma_2)\n",
    "    term4 = -alpha**(-4) * xi**3 * x**(gamma_1 + 3*gamma_2)\n",
    "    term5 = alpha**(-5) * xi**4 * x**(gamma_1 + 4*gamma_2)\n",
    "    term6 = -alpha**(-6) * xi**5 * x**(gamma_1 + 5*gamma_2)\n",
    "    \n",
    "    # Sum of terms\n",
    "    return term1 + term2 + term3 + term4 + term5 + term6\n",
    "\n",
    "def likelihood_transform(theta):\n",
    "   \n",
    "    xi, alpha,gamma_1,gamma_2 = theta # comment this line if you need two measuremnts.\n",
    "    \n",
    "    y_th = f_approx(data_x, gamma_1, gamma_2, alpha, xi)\n",
    "    \n",
    "    prob = np.sum(np.log(1/(uncertainty*(np.sqrt(2*np.pi))**2)*np.exp(-np.power(y_th-data_y, 2.)/(2*np.power(uncertainty,2.)))))\n",
    "    \n",
    "    if prob == -np.inf:\n",
    "        prob = -1e101\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultranest\n",
    "import ultranest.stepsampler\n",
    "parameters = ['xi','alpha','gamma_1','gamma_2']\n",
    "step = 2 * len(parameters)\n",
    "live_points = 50000\n",
    "\n",
    "max_calls = 60000000\n",
    "sampler = ultranest.ReactiveNestedSampler(parameters, likelihood_transform,prior_transform,log_dir='output')\n",
    "sampler.stepsampler = ultranest.stepsampler.SliceSampler(\n",
    "        nsteps=step, \n",
    "        generate_direction=ultranest.stepsampler.generate_mixture_random_direction,\n",
    "        # adaptive_nsteps=False,\n",
    "        # max_nsteps=400\n",
    "    )\n",
    "\n",
    "result = sampler.run(min_num_live_points=live_points,max_ncalls= max_calls)\n",
    "flat_samples = sampler.results['samples']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import corner\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Generate corner plot\n",
    "figure = corner.corner(\n",
    "    np.array(flat_samples),\n",
    "    labels=parameters, \n",
    "    show_titles=True,\n",
    "    title_kwargs={\"fontsize\": 12},\n",
    "    title_fmt = '.4f',\n",
    "    quantiles=[0.16, 0.5, 0.84],  # Show the 16th, 50th, and 84th percentiles  # Enable plotting of individual data points\n",
    "    fill_contours=True, \n",
    "    plot_datapoints=False,\n",
    "    bins=20,  # Increase the number of bins\n",
    "    smooth=0.9,  # Enable KDE smoothing\n",
    "    alpha=0.8  # Set alpha for better visualization of dense regions\n",
    ")\n",
    "\n",
    "# Save and display the plot\n",
    "#plt.savefig(\"posterior_J0030_condier_jrho_relation.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultranest\n",
    "import ultranest.stepsampler\n",
    "parameters = ['k','a1','b1']\n",
    "step = 2 * len(parameters)\n",
    "live_points = 50000\n",
    "\n",
    "max_calls = 60000000\n",
    "sampler = ultranest.ReactiveNestedSampler(parameters, likelihood_transform,prior_transform,log_dir='output')\n",
    "sampler.stepsampler = ultranest.stepsampler.SliceSampler(\n",
    "        nsteps=step,\n",
    "        generate_direction=ultranest.stepsampler.generate_mixture_random_direction,\n",
    "        # adaptive_nsteps=False,\n",
    "        # max_nsteps=400\n",
    "    )\n",
    "\n",
    "result = sampler.run(min_num_live_points=live_points,max_ncalls= max_calls)\n",
    "flat_samples = sampler.results['samples']\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
