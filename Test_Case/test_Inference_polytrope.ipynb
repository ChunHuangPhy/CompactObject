{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian inference analysis notebook\n",
    "\n",
    "This is an example notebook about how to use our tools to analysis a observation constraint on neutron star equation of state. \n",
    "\n",
    "Here in this notebook, we are using a polytrope EoS model (in the future we will implement more equation of state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a9/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import InferenceWorkflow.prior as prior\n",
    "import numpy as np\n",
    "import ultranest\n",
    "import ultranest.stepsampler\n",
    "\n",
    "import EOSgenerators.Polytrope_EOS as Polytrope\n",
    "from TOVsolver.maxium_central_density import maxium_central_density\n",
    "from TOVsolver.solver_code import solveTOV\n",
    "from TOVsolver.unit import g_cm_3, dyn_cm_2, km, Msun, e0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the outer crust, this crust and interface is unversial for all the equation of state here, we just change the inner crust part and the core part of equation of state and that is come from the polytrope computation. Crust as below, \"Tolos_crust_out.txt\" is BPS crust model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tolos_crust_out = np.loadtxt('Tolos_crust_out.txt', delimiter='  ')\n",
    "Tolos_crust_out = np.loadtxt('Tolos_crust_out.txt', delimiter=None, comments='#', usecols=(0, 1, 2, 3, 4))\n",
    "eps_crust = Tolos_crust_out[:,3] * g_cm_3\n",
    "pres_crust = Tolos_crust_out[:,4] * dyn_cm_2\n",
    "\n",
    "# rho_crust_end and P_crust_end are the end of outer crust EoS and the start point of polytrope EoS\n",
    "rho_crust_end = eps_crust[-1]\n",
    "P_crust_end = pres_crust[-1]\n",
    "\n",
    "# eps_core is the density array from inner crust to core, is the palce where we want to establish the polytrope EoS\n",
    "# cent_densitys is the central density range of a neutron star\n",
    "eps_core = np.logspace(11.7, 15.6, 1000, base=10) * g_cm_3\n",
    "cent_densitys = np.logspace(14.3, 15.6, 30) * g_cm_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up prior\n",
    "\n",
    "Next step, we need to set up the prior, first use parameters array to specify the variable name, should be consistent with what you need to call them. Second, we need to define a list to store some intermediate parameters derived from `prior_transform`. These parameters may be used in `likelihood_transform` or somewhere else and it will be stored in the final result, specifically 'd_max' in this case.\n",
    "    \n",
    "Define a prior transform function to define prior. Cube are set of random number from 0 to 1. This prior setting is standard set-up of UltraNest package, since we are using `UltraNest` to do nest-sampling. We provided \n",
    "\n",
    "`normal_Prior` and `flat_prior`\n",
    "\n",
    "two options call from prior. Here then the Parameters prior should all set\n",
    "\n",
    "------------------\n",
    "\n",
    "However, since we are doing Equation of state Inference from mass radius of neutron star measurement. The center density of the star should be also sampled. Otherwise will be a partially-defined prior, did not span all parameters space, and proved to be different with full-scope inference.\n",
    "\n",
    "This request as randomly generate a density from a EoS range, however, this process is not that trivial, since we need to determine the upper limit of the central density of neutron star ---  different equation of state will predict different upper bound, so here we need to use the prior-setting EoS parameters computing the EOS by\n",
    "\n",
    "```sh\n",
    "Polytrope.compute_EOS\n",
    "```\n",
    "\n",
    "Compute out EOS, put into\n",
    "\n",
    "```sh\n",
    "TOVsolver.maxium_central_density\n",
    "```\n",
    "\n",
    "find out Mass Radius of this equation of state, find out the last stable point of this equation of state.(first mass points that give the direvative to be negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = ['rho1', 'rho2', 'rho3', 'gamma1', 'gamma2', 'gamma3', 'gamma4', 'd1', 'd2', 'd3']\n",
    "derived_param_names = ['d_max']\n",
    "\n",
    "gamma_lo_ranges = [1, 0.1, 0.1, 0.1]\n",
    "gamma_up_ranges = [1.5, 4, 8, 4]\n",
    "def prior_transform(cube):\n",
    "    para = cube.copy()\n",
    "\n",
    "    # Output appropriate gamma\n",
    "    # When we randomly scatter a gamma at one segement, its maxium value is constrained by the start point's pressure, density,\n",
    "    # and the end point's density. Therefore in each segement, we need call 'fun_gamma_max' to get a sutiable range of gamma.\n",
    "    para[0] = prior.flat_prior(0.04/0.16*e0, 0.065/0.16*e0, cube[0])\n",
    "    para[1] = prior.flat_prior(1.5*e0, 8.3*e0, cube[1])\n",
    "    para[2] = prior.flat_prior(para[1], 8.3*e0, cube[2])\n",
    "    rho_s = [para[0], para[1], para[2], eps_core[-1]]\n",
    "    gamma_s = []\n",
    "    k = 0\n",
    "    pt = 0\n",
    "    for i in range(4):\n",
    "        if i==0:\n",
    "            gamma_max = Polytrope.fun_gamma_max(rho_s[i], rho_crust_end, P_crust_end)\n",
    "            if gamma_up_ranges[i] < gamma_max:\n",
    "                para[3+i] = prior.flat_prior(gamma_lo_ranges[i], gamma_up_ranges[i], cube[3+i])\n",
    "            else:\n",
    "                para[3+i] = prior.flat_prior(gamma_lo_ranges[i], gamma_max, cube[3+i])\n",
    "            gamma_s.append(para[3+i])\n",
    "            k = P_crust_end / (rho_crust_end**gamma_s[-1])\n",
    "            pt = k*rho_s[0]**gamma_s[-1]\n",
    "        else:\n",
    "            gamma_max = Polytrope.fun_gamma_max(rho_s[i], rho_s[i-1], pt)\n",
    "            if gamma_up_ranges[i] < gamma_max:\n",
    "                para[3+i] = prior.flat_prior(gamma_lo_ranges[i], gamma_up_ranges[i], cube[3+i])\n",
    "            else:\n",
    "                para[3+i] = prior.flat_prior(gamma_lo_ranges[i], gamma_max, cube[3+i])\n",
    "            gamma_s.append(para[3+i])\n",
    "            k = pt / (rho_s[i-1]**gamma_s[-1])\n",
    "            pt = k*rho_s[i]**gamma_s[-1]\n",
    "\n",
    "    gammas = np.array([para[3],para[4],para[5],para[6]])\n",
    "    rho_ts= np.array([para[0],para[1],para[2]])\n",
    "    theta = np.append(gammas, rho_ts)\n",
    "    pres = Polytrope.compute_EOS(eps_core, theta)\n",
    "    eps = np.hstack((eps_crust, eps_core))\n",
    "    pres = np.hstack((pres_crust, pres))\n",
    "    \n",
    "    d_max = maxium_central_density(eps, pres, cent_densitys)\n",
    "    para[7:] = 14.3 + (np.log10(d_max / g_cm_3) - 14.3) * cube[7:]\n",
    "\n",
    "    return np.append(para, d_max) # d_max is in natural unit, but d1,d2,d3 are the result of taking the logarithm of density in g_cm3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up likelihood\n",
    "\n",
    "We need to set up a likelihood, Using standard definition way of UltraNest, that is below.\n",
    "\n",
    "Here the likelihood is generated from a simulated mass radius measurement, which is $M = 1.97, 1.44, 2.07$ $M_{\\odot}$ and $R = 10, 13, 12$ km, With a 5% Mass radius measurement uncertainty, so here \n",
    "\n",
    "   ```sh\n",
    "   likelihood.MRlikihood_Gaussian\n",
    "   ```\n",
    "function will be use for our likelihood, please check [likelihood.MRlikihood_Gaussian](https://github.com/ChunHuangPhy/EoS_inference/blob/main/InferenceWorkflow/Likelihood.py) to see the original code, and more choice of likelihood.\n",
    "eg:\n",
    "1. If we have some real mass-radius measurements, say PSR J0030 or PSR J0740, come from NICER, a KDE kernel could be trained to feed into \n",
    "\n",
    "   ```sh\n",
    "   likelihood.MRlikihood_kernel(eps_total,pres_total,x,d1)\n",
    "   ```\n",
    "set the KDE kernel as a input for this function\n",
    "\n",
    "2. If we gain measurement from radio-timing, say only measure the neutron star mass, then\n",
    "\n",
    "   ```sh\n",
    "   likelihood.Masslikihood_Gaussian(eps_total,pres_total,x,d1)\n",
    "   ```\n",
    "Which will give the likelihood from single mass measurement, x is the parameters of that measurement, you should specify where this measurement mass is located and what is the sigma width of this mass measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "datas = [\n",
    "    [10, 1.97, 0.5, 0.0985],\n",
    "    [13, 1.44, 0.65, 0.072],\n",
    "    [12, 2.07, 0.6, 0.1035]\n",
    "]\n",
    "datas = pd.DataFrame(datas, index=[1,2,3], columns=['R', 'M', 'Rsigma', 'Msigma'])\n",
    "\n",
    "\n",
    "Rvalue = datas['R'].values * km\n",
    "Mvalue = datas['M'].values * Msun\n",
    "\n",
    "R_sigma = datas['Rsigma'].values * km\n",
    "M_sigma = datas['Msigma'].values * Msun\n",
    "\n",
    "f1 = np.log(1/(R_sigma*M_sigma*(np.sqrt(2*np.pi))**2)).sum()\n",
    "\n",
    "def from_para_to_MR_points(theta, densitys):\n",
    "    pres = Polytrope.compute_EOS(eps_core, theta)\n",
    "    eps = np.hstack((eps_crust, eps_core))\n",
    "    pres = np.hstack((pres_crust, pres))\n",
    "    unique_pressure_indices = np.unique(pres, return_index=True)[1]\n",
    "    unique_pressure = pres[np.sort(unique_pressure_indices)]\n",
    "\n",
    "    eos = interp1d(eps, pres, kind='cubic', fill_value='extrapolate')\n",
    "    inveos = interp1d(unique_pressure, eps[unique_pressure_indices], kind='cubic', fill_value='extrapolate')\n",
    "\n",
    "    MR_points = []\n",
    "    for dens in densitys:\n",
    "        try:\n",
    "            result = solveTOV(dens, pres[20], eos, inveos)\n",
    "            MR_points.append(result)\n",
    "        except OverflowError as e:\n",
    "            MR_points.append([0,0])\n",
    "    return np.array(MR_points)\n",
    "\n",
    "def likelihood_transform(theta):\n",
    "    gammas = np.array([theta[3],theta[4],theta[5],theta[6]])\n",
    "    rho_ts = np.array([theta[0],theta[1],theta[2]])\n",
    "    d_s = 10 ** np.array(theta[7:-1]) * g_cm_3\n",
    "\n",
    "    theta = np.append(gammas, rho_ts)\n",
    "    Ms, Rs = from_para_to_MR_points(theta, d_s).T\n",
    "\n",
    "    # In order to drop the unpysical parameters more efficiently, I used super Gaussian function here.\n",
    "    fm = np.where(Ms>3*Msun, -(Ms-2*Msun)**20, -(Ms-Mvalue)**2/(2*M_sigma**2))\n",
    "    fr = np.where(Rs>20*km, -(Rs-16*km)**20, -(Rs-Rvalue)**2/(2*R_sigma**2))\n",
    "\n",
    "    f2 = (fm + fr).sum()\n",
    "    likelihood = f1 + f2\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up sampler\n",
    "\n",
    "Here next, we define sampler.\n",
    "\n",
    "Some of the sampler parameters is requested, first is step number, our choice for UltraNest sampler is slicesampler, which could easily be sliced up your total computation load, and parallelize, speed up sampling. So step as suggested by documentation of UltraNest, we use 2*len(parameters).\n",
    "\n",
    "live_point we set 4000, it will influence the sampling precision, We suggest for 10 dimension space, maybe 5000 is a better choice, however, since my computer only have limited resources, we set 4000.\n",
    "\n",
    "max_calls set 500000, it is how many iteration after it will stop, we suggest to set this number significantly higher, maybe 5000000, otherwise maybe will broken before the inference converging to a definite value. That result will be un-phyiscal.\n",
    "\n",
    "derived_param_names is calculated from prior_transform and is the intermediate quantity necessary for calculating likelihood_transform. It will not be involved in the inference process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory for new run output/run22\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'chrip_mass' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32m/Users/a9/EoS_inference-1/Test_Case/test_Inference.ipynb Cell 10\u001b[0m line \u001b[0;36m5\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/a9/EoS_inference-1/Test_Case/test_Inference.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m live_point \u001b[39m=\u001b[39m \u001b[39m2000\u001b[39m\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell:/Users/a9/EoS_inference-1/Test_Case/test_Inference.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m max_calls \u001b[39m=\u001b[39m \u001b[39m60000\u001b[39m\n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/a9/EoS_inference-1/Test_Case/test_Inference.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m samples \u001b[39m=\u001b[39m sampler\u001b[39m.\u001b[39;49mUltranestSampler(parameters,likelihood_transform,prior_transform,step,live_point,max_calls)\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/codeastro/lib/python3.10/site-packages/InferenceWorkflow/BayesianSampler.py:24\u001b[0m, in \u001b[0;36mUltranestSampler\u001b[0;34m(parameters, likelihood, prior, step, live_points, max_calls)\u001b[0m\n",
      "\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mUltranestSampler\u001b[39m(parameters,likelihood,prior,step,live_points,max_calls):\n",
      "\u001b[1;32m      5\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"UltraNest based nested sampler by given likelihood prior, and parameters.\u001b[39;00m\n",
      "\u001b[1;32m      6\u001b[0m \u001b[39m    \u001b[39;00m\n",
      "\u001b[1;32m      7\u001b[0m \u001b[39m    Args:\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m     22\u001b[0m \u001b[39m        \u001b[39;00m\n",
      "\u001b[1;32m     23\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m---> 24\u001b[0m     sampler \u001b[39m=\u001b[39m ultranest\u001b[39m.\u001b[39;49mReactiveNestedSampler(parameters, likelihood, prior,log_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39moutput\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m     25\u001b[0m     sampler\u001b[39m.\u001b[39mstepsampler \u001b[39m=\u001b[39m ultranest\u001b[39m.\u001b[39mstepsampler\u001b[39m.\u001b[39mSliceSampler(\n",
      "\u001b[1;32m     26\u001b[0m         nsteps\u001b[39m=\u001b[39mstep,\n",
      "\u001b[1;32m     27\u001b[0m         generate_direction\u001b[39m=\u001b[39multranest\u001b[39m.\u001b[39mstepsampler\u001b[39m.\u001b[39mgenerate_mixture_random_direction,\n",
      "\u001b[1;32m     28\u001b[0m         \u001b[39m# adaptive_nsteps=False,\u001b[39;00m\n",
      "\u001b[1;32m     29\u001b[0m         \u001b[39m# max_nsteps=400\u001b[39;00m\n",
      "\u001b[1;32m     30\u001b[0m     )\n",
      "\u001b[1;32m     32\u001b[0m     result \u001b[39m=\u001b[39m sampler\u001b[39m.\u001b[39mrun(min_num_live_points\u001b[39m=\u001b[39mlive_points,max_ncalls\u001b[39m=\u001b[39m max_calls)\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/codeastro/lib/python3.10/site-packages/ultranest/integrator.py:1196\u001b[0m, in \u001b[0;36mReactiveNestedSampler.__init__\u001b[0;34m(self, param_names, loglike, transform, derived_param_names, wrapped_params, resume, run_num, log_dir, num_test_samples, draw_multiple, num_bootstraps, vectorized, ndraw_min, ndraw_max, storage_backend, warmstart_max_tau)\u001b[0m\n",
      "\u001b[1;32m   1194\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndraw_max \u001b[39m=\u001b[39m ndraw_max\n",
      "\u001b[1;32m   1195\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuild_tregion \u001b[39m=\u001b[39m transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;32m-> 1196\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_likelihood_function(transform, loglike, num_test_samples):\n",
      "\u001b[1;32m   1197\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_to_disk\n",
      "\u001b[1;32m   1198\u001b[0m     \u001b[39mif\u001b[39;00m resume_similar \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_to_disk:\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/codeastro/lib/python3.10/site-packages/ultranest/integrator.py:1258\u001b[0m, in \u001b[0;36mReactiveNestedSampler._check_likelihood_function\u001b[0;34m(self, transform, loglike, num_test_samples)\u001b[0m\n",
      "\u001b[1;32m   1254\u001b[0m p \u001b[39m=\u001b[39m transform(u) \u001b[39mif\u001b[39;00m transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m u\n",
      "\u001b[1;32m   1255\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39mshape(p) \u001b[39m==\u001b[39m (num_test_samples, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_params), (\n",
      "\u001b[1;32m   1256\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mError in transform function: returned shape is \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, expected \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\n",
      "\u001b[1;32m   1257\u001b[0m         np\u001b[39m.\u001b[39mshape(p), (num_test_samples, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_params)))\n",
      "\u001b[0;32m-> 1258\u001b[0m logl \u001b[39m=\u001b[39m loglike(p)\n",
      "\u001b[1;32m   1259\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39mlogical_and(u \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, u \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mall(), (\n",
      "\u001b[1;32m   1260\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mError in transform function: u was modified!\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m   1261\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39mshape(logl) \u001b[39m==\u001b[39m (num_test_samples,), (\n",
      "\u001b[1;32m   1262\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mError in loglikelihood function: returned shape is \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, expected \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (np\u001b[39m.\u001b[39mshape(logl), (num_test_samples,)))\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/codeastro/lib/python3.10/site-packages/ultranest/utils.py:134\u001b[0m, in \u001b[0;36mvectorize.<locals>.vectorized\u001b[0;34m(args)\u001b[0m\n",
      "\u001b[1;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvectorized\u001b[39m(args):\n",
      "\u001b[1;32m    133\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Vectorized version of function.\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray([function(arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args])\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/codeastro/lib/python3.10/site-packages/ultranest/utils.py:134\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[1;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvectorized\u001b[39m(args):\n",
      "\u001b[1;32m    133\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Vectorized version of function.\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray([function(arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args])\n",
      "\n",
      "\u001b[1;32m/Users/a9/EoS_inference-1/Test_Case/test_Inference.ipynb Cell 10\u001b[0m line \u001b[0;36m6\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/a9/EoS_inference-1/Test_Case/test_Inference.ipynb#X12sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m chrip170817 \u001b[39m=\u001b[39m stats\u001b[39m.\u001b[39mgaussian_kde(GW170817[:,\u001b[39m0\u001b[39m],weights \u001b[39m=\u001b[39m GW170817[:,\u001b[39m4\u001b[39m])\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/a9/EoS_inference-1/Test_Case/test_Inference.ipynb#X12sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m kernelGW \u001b[39m=\u001b[39m stats\u001b[39m.\u001b[39mgaussian_kde(GW170817\u001b[39m.\u001b[39mT, weights \u001b[39m=\u001b[39m GW170817[:,\u001b[39m4\u001b[39m])\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/a9/EoS_inference-1/Test_Case/test_Inference.ipynb#X12sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m probGW \u001b[39m=\u001b[39m likelihood\u001b[39m.\u001b[39;49mTidalLikihood_kernel(eps_total,pres_total,(kernelGW,chrip170817),d1)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/a9/EoS_inference-1/Test_Case/test_Inference.ipynb#X12sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m prob \u001b[39m=\u001b[39m  probGW\u001b[39m#+ probMRJ0030 + probK + probJ + probL + probGW\u001b[39;00m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell:/Users/a9/EoS_inference-1/Test_Case/test_Inference.ipynb#X12sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39mreturn\u001b[39;00m prob\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/codeastro/lib/python3.10/site-packages/InferenceWorkflow/Likelihood.py:67\u001b[0m, in \u001b[0;36mTidalLikihood_kernel\u001b[0;34m(eps_total, pres_total, x, d1)\u001b[0m\n",
      "\u001b[1;32m     65\u001b[0m \u001b[39mif\u001b[39;00m   \u001b[39mall\u001b[39m(x\u001b[39m<\u001b[39my \u001b[39mfor\u001b[39;00m x,y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(eps_total[:], eps_total[\u001b[39m1\u001b[39m:])) \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(x\u001b[39m<\u001b[39my \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(pres_total[:], pres_total[\u001b[39m1\u001b[39m:])):\n",
      "\u001b[1;32m     66\u001b[0m     MRT \u001b[39m=\u001b[39m main\u001b[39m.\u001b[39mOutputMRTpoint(d1,eps_total,eps_total)\u001b[39m.\u001b[39mT\n",
      "\u001b[0;32m---> 67\u001b[0m     M1 \u001b[39m=\u001b[39m TOV_solver\u001b[39m.\u001b[39mm1_from_mc_m2(chrip_mass, MRT[\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m])\n",
      "\u001b[1;32m     68\u001b[0m     Tidal_line \u001b[39m=\u001b[39m main\u001b[39m.\u001b[39mOutputMRT(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m,eps_total,eps_total)\u001b[39m.\u001b[39mT\n",
      "\u001b[1;32m     69\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(MRT[\u001b[39m0\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(Tidal_line[\u001b[39m0\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m:\n",
      "\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'chrip_mass' referenced before assignment"
     ]
    }
   ],
   "source": [
    "step = 2 * len(parameters)\n",
    "live_point = 4000 # More parameters need more points.\n",
    "max_calls = 500000 # In the example code, the result is Z=-5.7(23.50%), which means that the max_calls should be larger.\n",
    "\n",
    "sampler = ultranest.ReactiveNestedSampler(parameters, likelihood_transform, prior_transform, log_dir='output/Polytrope_EOS', derived_param_names=derived_param_names)\n",
    "sampler.stepsampler = ultranest.stepsampler.SliceSampler(\n",
    "    nsteps=step,\n",
    "    generate_direction=ultranest.stepsampler.generate_mixture_random_direction,\n",
    "    # adaptive_nsteps=False,\n",
    "    # max_nsteps=400\n",
    ")\n",
    "\n",
    "result = sampler.run(min_num_live_points=live_point,max_ncalls= max_calls)\n",
    "flat_samples = sampler.results['samples']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
